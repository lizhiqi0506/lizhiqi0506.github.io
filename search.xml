<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>编程小记-&quot;@&quot;运算符的作用</title>
    <url>/2022/01/13/2022-01-13-%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%AE%B0-@%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<p>在看PyG的RGCNConv的代码时，看到其前向传播中有这么一段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = self.weight</span><br><span class="line"><span class="keyword">if</span> self.num_bases <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># Basis-decomposition =================</span></span><br><span class="line">	weight = (self.comp @ weight.view(self.num_bases, -<span class="number">1</span>)).view(</span><br><span class="line">		self.num_relations, self.in_channels_l, self.out_channels)</span><br></pre></td></tr></table></figure>
<p>这里有个@符号，不知道是什么意思。之前只知@xxx是注解或装饰器，但显然不是这段代码的场景，这里@更像是一个运算符，其前后的两个变量都是矩阵，于是猜测@在这里是矩阵运算相关的功能。</p>
<p>Google之后得知，Python 3.5之后，@可以作为矩阵乘法运算符，是两个矩阵直接的叉乘，<code>A @ B</code>等同于<code>np.dot(A, B)</code>以及<code>tf.matmul(A, B)</code></p>
<p><img src="/2022/01/13/2022-01-13-%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%AE%B0-@%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8/编程小记-@运算符的作用.png" style="zoom: 150%;"></p>
]]></content>
      <categories>
        <category>编程小记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title>MVG-学习笔记-射影几何基础概念</title>
    <url>/2022/01/09/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%87%A0%E4%BD%95%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="2D射影平面"><a href="#2D射影平面" class="headerlink" title="2D射影平面"></a>2D射影平面</h2><h3 id="点与直线"><a href="#点与直线" class="headerlink" title="点与直线"></a>点与直线</h3><h4 id="直线的齐次表示"><a href="#直线的齐次表示" class="headerlink" title="直线的齐次表示"></a>直线的齐次表示</h4><p>平面上的一条直线可以用$ax+by+c=0$表示，一组$(a,b,c)$的值唯一确定一条直线，因此，一条直线也可以用$(a,b,c)^{\text{T}}$表示，同时，对于任何非零常数$k$，直线$(a,b,c)^\text{T}$与直线$k(a,b,c)^\text{T}$表示同一条直线。我们视这两个只相差一个全局缩放因子的向量是等价的，这种等价关系下的向量等价类被称为<strong>齐次</strong>向量，上述即为直线的齐次表示。</p>
<p>$\text{IR}^3-(0,0,0)^\text{T}$中的向量等价类的集合组成射影空间$\text{IP}^2$</p>
<h4 id="点的齐次表示"><a href="#点的齐次表示" class="headerlink" title="点的齐次表示"></a>点的齐次表示</h4><p>点$\pmb{\text{x}}=(x,y)^\text{T}$在直线$\pmb{\text{l}}=(a,b,c)^\text{T}$上的充要条件是：</p>
<script type="math/tex; mode=display">
ax+by+c=0</script><p>即：</p>
<script type="math/tex; mode=display">
(x,y,1)(a,b,c)^\text{T}=(x,y,1)\pmb{\text{l}}=0</script><p>即通过添加最后一个坐标$“1”$，将$\text{IR}^2$（2维欧氏空间）中的点$\pmb{\text{x}}$表示为3维向量。</p>
<p>而对任意非零$k$，方程$(kx,ky,k)\pmb{\text{l}}=0$当且仅当$(x,y,1)\pmb{\text{l}}=0$。因而，可自然地把$k$取不同非零值构成的向量集$(kx,ky,k)^\text{T}$看作是$\text{IR}^2$中点$(x,y)^\text{T}$的一种表示，因此，与直线一样，点也可用齐次向量表示。</p>
<p>即，任意$\text{IR}^2$中的点$(x,y)^\text{T}$，其$\text{IP}^2$中的齐次表示为$(kx,ky,k)^\text{T}(k\neq0)$</p>
<h4 id="三个结论"><a href="#三个结论" class="headerlink" title="三个结论"></a>三个结论</h4><ul>
<li><p>点$\pmb{\text{x}}$在直线$\pmb{\text{l}}$上当且仅当$\pmb{\text{x}}^\text{T}\pmb{\text{l}}=0$</p>
</li>
<li><p>两直线$\pmb{\text{l}}$和$\pmb{\text{l}}’$的交点是点$\pmb{\text{x}=\text{l}\times \text{l}’}$</p>
</li>
<li><p>过点$\pmb{\text{x}}$和点$\pmb{\text{x}}’$的直线是$\pmb{\text{l}=\text{x}\times \text{x}’}$</p>
</li>
</ul>
<h3 id="理想点与无穷远直线"><a href="#理想点与无穷远直线" class="headerlink" title="理想点与无穷远直线"></a>理想点与无穷远直线</h3><h4 id="平行线的交点——理想点"><a href="#平行线的交点——理想点" class="headerlink" title="平行线的交点——理想点"></a>平行线的交点——理想点</h4><p>考察两平行直线：$ax+by+c=0$和$ax+by+c’=0$，它们分别用向量$\pmb{\text{l}}=(a,b,c)^\text{T}$和$\pmb{\text{l}}’=(a,b,c’)^\text{T}$表示。用之前得到的结论不难算出它们的交点为$\pmb{\text{l}}\times\pmb{\text{l}}’=(c’-c)(b,-a,0)^\text{T}$，忽略标量因子$(c’-c)$，可得到点$(b,-a,0)^\text{T}$。如果我们试图求这一点的非齐次表示，就会得到$(b/0, -a/0)^\text{T}$，这并没有什么实际意义，除了我们可以将其解释为无穷大坐标。</p>
<p>实际上，在欧式空间中无法表示的无穷远点（除非使用无穷大符号，而即便这样也无法用其进行数学运算），可在射影空间中用$(x,y,0)^\text{T}$进行齐次表示，并称这些点为<strong>理想点</strong>，或无穷远点。</p>
<h4 id="无穷远直线"><a href="#无穷远直线" class="headerlink" title="无穷远直线"></a>无穷远直线</h4><p>射影空间中所有理想点的集合可以写成$(x_1,x_2,0)^\text{T}$，并由比率$x_1:x_2$指定一个具体的点。</p>
<p>该点集在直线$\pmb{\text{l}}_\infty=(0,0,1)^\text{T}$表示的一条直线（即<strong>无穷远直线</strong>）上。</p>
<p>对于直线$\pmb{\text{l}}=(a,b,c)^\text{T}$，其与$\pmb{\text{l}}_\infty$交于理想点$(b,-a,0)^\text{T}$，而对于任意一条与$\pmb{\text{l}}$平行的直线$\pmb{\text{l}}’=(a,b,c’)^\text{T}$，其也与$\pmb{\text{l}}_\infty$交于相同的理想点，而与$c’$ 的取值无关。</p>
<p>在非齐次表示下，向量$(b,-a)^\text{T}$是与直线$\pmb{\text{l}}$（或$\pmb{\text{l}}’$）相切的向量，与该直线的法线相正交，因而它代表该直线的方向。当直线的方向改变，即比率$b:a$发生变化时，理想点$(b,-a,0)^\text{T}$也在沿$\pmb{\text{l}}_\infty$而变化。因此，<strong>无穷远直线并不是一条真正的直线，它是一个抽象的概念，可以看作是平面上所有直线方向的集合</strong>，而同一方向的所有平行线都相交于无穷远直线上的同一点。</p>
<h4 id="引入理想点和无穷远直线的意义"><a href="#引入理想点和无穷远直线的意义" class="headerlink" title="引入理想点和无穷远直线的意义"></a>引入理想点和无穷远直线的意义</h4><p>理想点和无穷远直线都是射影空间中的定义，在射影空间$\text{IP}^2$中，我们可以不假思索地说任意两条相异直线都相交（任意两条相异的平行线相交于无穷远直线上的一点），而任意两个相异的点都在一条直线上，但这些在标准的欧氏几何$\text{IR}^2$中就不成立。</p>
<p>研究射影空间的几何成为射影几何。在无坐标的纯几何研究中，射影几何的无穷远点和普通点没有任何区别。但在实际问题中，无穷远点相比普通点还是比较特殊的，因而在实际研究中，我们总应额外考虑无穷远点和无穷远直线。</p>
]]></content>
      <tags>
        <tag>计算机视觉</tag>
        <tag>多视图几何</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/12/18/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>MVG-学习笔记-射影变换</title>
    <url>/2022/01/10/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2/</url>
    <content><![CDATA[<h2 id="射影变换（Projective-Transformation）"><a href="#射影变换（Projective-Transformation）" class="headerlink" title="射影变换（Projective Transformation）"></a>射影变换（Projective Transformation）</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><h4 id="定义一"><a href="#定义一" class="headerlink" title="定义一"></a>定义一</h4><p><strong>射影变换</strong>是$\text{IP}^2$到它自身的一种满足下列条件的可逆映射$h$: 三点$\pmb{\text{x}}_1$, $\pmb{\text{x}}_2$, $\pmb{\text{x}}_3$三点共线当且仅当$h(\pmb{\text{x}}_1)$, $h(\pmb{\text{x}}_2)$, $h(\pmb{\text{x}}_3)$也共线。</p>
<p>射影变换组成一个群，即射影变换的逆以及多个射影变换的复合仍是射影变换。</p>
<h4 id="定义二"><a href="#定义二" class="headerlink" title="定义二"></a>定义二</h4><p>映射$h$: $\text{IP}^2 \to \text{IP}^2$是射影变换的充要条件是：存在一个3x3的非奇异矩阵$\text{H}$，使得$\text{IP}^2$的任何一个用向量$\pmb{\text{x}}$表示的点都满足$h(\pmb{\text{x}})=\text{H}\pmb{\text{x}}$.</p>
<h4 id="定义三"><a href="#定义三" class="headerlink" title="定义三"></a>定义三</h4><p>一个平面射影变换是关于3维齐次向量的一种线性变换，并可以用一个非奇异3x3矩阵表示为：</p>
<script type="math/tex; mode=display">
\left(
\begin{matrix}
x'_1 \\
x'_2 \\
x'_3
\end{matrix}
\right)
=
\left[\begin{matrix}
h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33}
\end{matrix}\right]
\left(
\begin{matrix}
x_1 \\
x_2 \\
x_3
\end{matrix}
\right)</script><p>或更简洁地表示为$\pmb{\text{x}}’=\text{H}\pmb{\text{x}}$</p>
<p>注：该方程中的矩阵$\text{H}$乘以任意一个非零的尺度因子都不会使射影变换改变，有意义的仅是矩阵元素的比率。在$\text{H}$的9个元素中有8个独立比率，因此一个射影变换有8个自由度。</p>
<h3 id="直线与二次曲线的变换"><a href="#直线与二次曲线的变换" class="headerlink" title="直线与二次曲线的变换"></a>直线与二次曲线的变换</h3><p>在点变换$\pmb{\text{x}}’=\text{H}\pmb{\text{x}}$下：</p>
<h4 id="直线的变换"><a href="#直线的变换" class="headerlink" title="直线的变换"></a>直线的变换</h4><p>如果点$\pmb{\text{x}}$在直线$\pmb{\text{l}}$上，那么$\pmb{\text{x}}^{\text{T}}\pmb{\text{l}}=0$，</p>
<p>假设变换后的点$\pmb{\text{x}}’$在直线$\pmb{\text{l}}’$上，那么有$\pmb{\text{x}}’^{\text{T}}\pmb{\text{l}}’=0$，</p>
<p>即$\pmb{\text{x}}^{\text{T}} \text{H}^{\text{T}}\pmb{\text{l}}’=0$，故而$\pmb{\text{l}}=\text{H}^{\text{T}}\pmb{\text{l}}’$，$\pmb{\text{l}}’=\text{H}^{-\text{T}}\pmb{\text{l}}$.</p>
<h4 id="二次曲线的变换"><a href="#二次曲线的变换" class="headerlink" title="二次曲线的变换"></a>二次曲线的变换</h4><p>之前我们知道，二次曲线可用矩阵形式$\pmb{\text{x}}^{\text{T}}\text{C}\pmb{\text{x}}=0$表示，</p>
<p>假设变换后的二次曲线的系数矩阵为$\text{C}’$，那么有$\pmb{\text{x}}’^\text{T}\text{C}’\pmb{\text{x}}’=0$，</p>
<p>即$\pmb{\text{x}}^\text{T} \text{H}^\text{T}\text{C}’\text{H}\pmb{\text{x}}=0$，故而$\text{C}=\text{H}^\text{T}\text{C}’\text{H}$，</p>
<p>所以$\text{C}’=\text{H}^{-\text{T}} \text{CH}^{-1}$</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>多视图几何</tag>
      </tags>
  </entry>
  <entry>
    <title>history_of_cv</title>
    <url>/2022/01/05/history-of-cv/</url>
    <content><![CDATA[<h3 id="计算机视觉的发展历史（from-1960s）"><a href="#计算机视觉的发展历史（from-1960s）" class="headerlink" title="计算机视觉的发展历史（from 1960s）"></a>计算机视觉的发展历史（from 1960s）</h3><hr>
<h4 id="大致的时间线"><a href="#大致的时间线" class="headerlink" title="大致的时间线"></a>大致的时间线</h4><ul>
<li>1960-1970: Blocks Worlds, Edges and Model Fitting</li>
<li>1970-1981: Low-level vision: stereo, flow, shape-from-shading</li>
<li>1985-1988: Neural networks, backpropagation, self-driving</li>
<li>1990-2000: Dense stereo and multi-view stereo, MRFs(马尔科夫随机场)</li>
<li>2000-2010: Features, descriptors, large-scale structure-from-motion</li>
<li>2010-now: Deep learning, large datasets, quick growth, commercialization</li>
</ul>
<p><img src="/2022/01/05/history-of-cv/overview-timeline.png" alt></p>
<h4 id="当前面临的挑战"><a href="#当前面临的挑战" class="headerlink" title="当前面临的挑战"></a>当前面临的挑战</h4><ul>
<li>Un-/Self-Supervised Learning</li>
<li>Interactive learning</li>
<li>Accuracy (e.g., self-driving)</li>
<li>Robustness and generalization</li>
<li>Inductive biases</li>
<li>Understanding and mathematics</li>
<li>Memory and compute</li>
<li>Ethics and legal questions</li>
</ul>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>主成分分析(PCA)原理解析</title>
    <url>/2022/03/23/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>主成分分析（Principal Component Analysis，PCA）是一种常见的数据分析算法，常用于高维数据的降维，也可用于提取数据的主要特征分量。</p>
<p>PCA的数学原理可以从“最大可分性”和“最近重构性”两方面来解释，这里我将从最大可分性的角度进行展开。</p>
<p>出于规范，本文中所有向量默认为列向量。</p>
<h3 id="坐标基与基变换"><a href="#坐标基与基变换" class="headerlink" title="坐标基与基变换"></a>坐标基与基变换</h3><p>我们知道，一个向量的值都是基于一组基存在的，若是基变了，那么该向量的值也会随之改变。</p>
<p>在一个二维平面中，假设一个向量$x=(3, 2)^T$，用二维平面我们默认的基$e_1=(1, 0)^T,\ e_2=(0,1)^T$表示则为：</p>
<script type="math/tex; mode=display">
x =3e_1+2e_2</script><p>设矩阵$E=(e_1, e_2)^T$为基的矩阵，那么上式可以表示为：$Ex$。</p>
<p>若是我们使用的基换做$E’=(e_1’, e_2’)^T$，其中$e_1’=(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T$，$e_2’=(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T$。这时候与上面同样的一个向量（该二维空间中同样的一个点）该如何表示呢？</p>
<p>实际上，将新的基矩阵与原基下的向量表示相乘即可得到新基下的向量表示：</p>
<script type="math/tex; mode=display">
x'=E'x=\left(\begin{matrix}
             5/\sqrt{2} \\
             -1/\sqrt{2} \\
             \end{matrix}
             \right)</script><p>上述结论可以扩展到N维空间中M个由其原基（其基矩阵是一个N维单位矩阵）表示的向量$X=\{x_1, x_2, …, x_M\}$，它们在一组个数为K的新基$E’$下（其基矩阵为一个$K\times N$的矩阵）新的向量表示为：</p>
<script type="math/tex; mode=display">
X'_{K\times M}=EX=\{x_1',x_2',...,x_M'\}</script><p>(关于上述内容原理的解析，笔者不再展开叙述，这里指路b站搜索《线性代数的本质》，时长两个小时左右，12小集，看完之后会对线性代数有一个更深的理解)</p>
<h3 id="最大可分性"><a href="#最大可分性" class="headerlink" title="最大可分性"></a>最大可分性</h3><p>在前一节我们知道，不同基的选取可以对同样的一组数据给出不同的表示，而如果新基的数量少于原基的数目的话，则可以起到降维的效果。</p>
<p>如果我们有一组数据，其中包含$M$个样本，每个样本的特征长度为$N$，即：$X_{N\times M}$，这里最好是经过标准化之后的数据，这时对于每一个特征，其值在不同样本之间$\mu=0, \sigma=1$。</p>
<p>我们想要将每个样本的特征长度降为$K$，那么，我们该如何选取$K$个新基来对这组数据进行变换，同时最大程度地保留原有的信息呢？</p>
<p>一种较为直观的看法是：令投影后的投影值尽可能地分散，当然也可以从熵的角度进行理解——熵越大所含的信息就越多。</p>
<h4 id="一个向量的分散程度——方差"><a href="#一个向量的分散程度——方差" class="headerlink" title="一个向量的分散程度——方差"></a>一个向量的分散程度——方差</h4><p><strong>方差</strong>是我们最常用的用来表示一维数据分散程度的一个统计量，那么我们就希望<strong>在投影之后，每一个向量的新的表示值拥有尽可能大的方差</strong>。</p>
<h4 id="向量之间的分散程度——协方差"><a href="#向量之间的分散程度——协方差" class="headerlink" title="向量之间的分散程度——协方差"></a>向量之间的分散程度——协方差</h4><p>在一维空间中我们可以用方差来表示数据的分散程度，而对于高维数据，我们引入协方差进行约束：</p>
<script type="math/tex; mode=display">
Cov(a,b)=\frac{1}{m-1}\sum_{i=1}^{m}(a_i-\mu_a)(b_i-\mu_b)</script><p>当均值为0时，上述公式可表示为：</p>
<script type="math/tex; mode=display">
Cov(a, b)=\frac{1}{m}\sum_{i=1}^{m}a_ib_i</script><p>当样本数较大时，为了方便计算，分母取m。</p>
<p>协方差可以表示两组变量的相关性。为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>当协方差为0时，表示两个向量不存在线性相关。为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。（这里存疑）</p>
<p>至此，我们得到了降维任务的优化目标：<strong>将一组$N$维向量降为$K$维，其目标是选择$K$个单位正交基，使得原始数据变换到这组基上后，各变量两两之间协方差为0，而变量内部的方差尽可能大</strong>。</p>
<p>对于原始数据$X$，设其协方差矩阵</p>
<script type="math/tex; mode=display">
C=\frac{1}{M}XX^T=\left(
                \begin{matrix}
                Cov(x_1, x_1) & Cov(x_1,x_2) & \cdots & Cov(x_1, x_N) \\
                Cov(x_2, x_1) & Cov(x_2,x_2) & \cdots & Cov(x_2, x_N) \\
                \vdots          & \vdots       & \ddots & \vdots          \\
                Cov(x_N, x_1) & Cov(x_N,x_2) & \cdots & Cov(x_N, x_N)
                \end{matrix}
                \right
                )</script><p>这个矩阵是一个对称矩阵，其对角线上是每个变量的方差，其他元素是各变量两两之间的协方差。</p>
<p>假设我们选取与之前相同数目的一组基的矩阵为$P_{N\times N}$，那么变换后新的特征表示为$Y_{N\times S}=P_{N\times N}X_{N\times S}$。</p>
<p>同样地，</p>
<script type="math/tex; mode=display">
\begin{aligned}
D&=\frac{1}{M}YY^T \\
 &=\frac{1}{M}(PX)(PX)^T \\
 &=\frac{1}{M}PXX^TP^T \\
 &=P(\frac{1}{M}XX^T)P^T \\
 &=PCP
\end{aligned}</script><p>根据上面提到的优化目标，我们希望$D$能够满足以下条件：</p>
<ul>
<li>是一个对角矩阵（不同变量两两之间协方差为0）</li>
<li>对角元素降序排列（按方差从小到大的顺序排列）</li>
</ul>
<p>也就是说，我们想要找的$P$是能够将$C$对角化的矩阵，由于$C$是一个实对称矩阵，由线性代数的相关知识我们可知，对于$C$的$N$个特征值，其单位正交化的特征向量为：$E=(e_1,e_2,…,e_N)$，那么</p>
<script type="math/tex; mode=display">
E^TCE=\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_N)</script><p>我们可以看到$\Lambda$其实就是满足条件的$D$，而$P=E^T$。</p>
<p>$P$ 是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是 $C$ 的一个特征向量。如果设 $P$ 按照$\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用 $P$ 的前 $K$ 行组成的矩阵乘以原始数据矩阵 $X$，就得到了我们需要的降维后的数据矩阵 $X’$.</p>
<h3 id="步骤总结"><a href="#步骤总结" class="headerlink" title="步骤总结"></a>步骤总结</h3><p>假设有$M$个样本，每个样本的特征长度为$N$，即：$X_{N\times M}$</p>
<ol>
<li>将X按行进行零均值化，即减去这一行的均值；</li>
<li>求出协方差矩阵$C=\frac{1}{M}XX^T$;</li>
<li>求出协方差矩阵的特征值及对应的单位正交化的特征向量；</li>
<li>将特征向量转置成行向量，按对应特征值从大到小的顺序取前$K$个组成矩阵$P_{K\times N}$;</li>
<li>$X’=PX$即为降维后的数据。</li>
</ol>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ol>
<li><strong>缓解维度灾难</strong>：降维的作用；</li>
<li><strong>降噪</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>可能会导致过拟合</strong>：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；</li>
<li><strong>特征独立</strong>：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立。</li>
</ol>
]]></content>
      <tags>
        <tag>线性代数</tag>
        <tag>特征降维</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络中各元件解析</title>
    <url>/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><h4 id="参数含义"><a href="#参数含义" class="headerlink" title="参数含义"></a>参数含义</h4><ul>
<li>in_channels: 输入特征图的通道数，如果是RGB图像，则通道数为3。卷积中的特征图通道数一般是2的整数次幂。</li>
<li>out_channels: 输出特征图的通道数。</li>
<li>kernel_size: 卷积核的尺寸，常见的有1，3，5，7.</li>
<li>stride: 步长，一般为1。如果大于1，则输出特征图的尺寸会小于输出特征图的尺寸。</li>
<li>padding：填充，填充方式有constant（常量填充，默认情况下是0填充），reflect（反射填充）、replicate（复制填充）、circular（循环填充）</li>
<li>dilation: 空洞卷积，当大于1时可以增大感受野的同时保持特征图的尺寸，默认为1。</li>
<li>groups: 组卷积，即在卷积操作时不是逐点卷积，而是将输入通道分为多个组，稀疏连接打到降低计算量的目的，默认为1.</li>
<li>bias：是否需要偏置。</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li><a href="https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer,">Dropout Paper</a></li>
</ul>
<p>2012年，Hinton等人提出了Dropout算法，可以比较有效地缓解过拟合现象，起到一定正则化的效果。</p>
<p>Dropout的基本思想如图所示，在训练时，每个神经元以概率p保留，即以1-p的概率停止工作，每次前向传播保留下来的神经元都不同，这样可以使得模型不太依赖于某些局部特征，泛化性能更强。在测试时，为了保证相同的输出期望值，每个参数还要乘以p。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/dropout.png" alt="Dropout"></p>
<p>为什么Dropout可以防止过拟合，可以从以下3个方面解释：</p>
<ul>
<li>多模型的平均：不同的固定神经网络会有不同的过拟合，多个取平均则有可能让一些相反的拟合抵消掉，而Dropout每次都是不同的神经元失活，可以看做是多个模型的平均，类似于多数投票取胜的策略。</li>
<li>减少神经元间的依赖：由于两个神经元不一定同时有效，因此减少了特征之间的依赖，迫使网络学习有更为鲁棒的特征，因为神经网络不应该对特定的特征敏感，而应该从众多特征中学习更为共同的规律，这也起到了正则化的效果。</li>
<li>生物进化：Dropout类似于性别在生物进化中的角色，物种为了适应环境变化，在繁衍时取雄性和雌性的各一半基因进行组合，这样可以适应更复杂的新环境，避免了单一基因的过拟合，当环境发生变化时也不至于灭绝。</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul>
<li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf">BN Paper</a></li>
</ul>
<p>由于网络中参数变化导致的内部节点数据分布发生变化的现象被称做ICS（Internal Covariate Shift）。ICS现象容易使训练过程陷入饱和区，减慢网络的收敛。前面提到的ReLU从激活函数的角度出发，在一定程度上解决了梯度饱和的现象，而2015年提出的BN层，则从改变数据分布的角度避免了参数陷入饱和区。由于BN层优越的性能，其已经是当前卷积网络中的“标配”。</p>
<h4 id="BN的过程"><a href="#BN的过程" class="headerlink" title="BN的过程"></a>BN的过程</h4><p>对于一个batch的输入$x: B=\{x_1, …, x_m\}$，其中$m$为batch的大小，BN首先对其进行白化操作，即去均值方差的过程。</p>
<script type="math/tex; mode=display">
\mu_B=\frac{1}{m}\sum_{i=1}^{m}x_i</script><script type="math/tex; mode=display">
\sigma^2_B = \frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_B)^2 \\</script><script type="math/tex; mode=display">
\hat{x}_i = \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}</script><p>其中$\epsilon$是为了数值稳定而添加的一个常数（论文原文是”a constant added to the mini-batch variance for numerical stability“，这里我也不明白为什么这样就可以使数值更稳定）</p>
<p>白化操作可以使输入的特征分布具有相同的均值与方差，固定了每一层的输入分布，从而加速网络的收敛。但是，白化操作也限制了网络中数据的表达能力，浅层学到的参数信息会被白化操作屏蔽掉，因此，BN层在白化操作后又增加了一个可学习的线性变换操作（引入可学习参数$\gamma$和$\beta$），让数据尽可能地恢复本身的表达能力：</p>
<script type="math/tex; mode=display">
y_i=\gamma\hat{x}_i+\beta</script><h4 id="BN的优点"><a href="#BN的优点" class="headerlink" title="BN的优点"></a>BN的优点</h4><ul>
<li>缓解梯度消失，加速网络收敛。BN层可以让激活函数的输入数据落在非饱和区，缓解了梯度消失问题。此外，由于每一层数据的均值与方差都在一定范围内，深层网络不必去不断适应浅层网络输入的变化，实现了层间解耦，允许每一层独立学习，也加快了网络的收敛。</li>
<li>简化调参，网络更稳定。在调参时，学习率调得过大容易出现震荡与不收敛，BN层则抑制了参数微小变化随网络加深而被放大的问题，因此对于参数变化的适应能力更强，更容易调参。</li>
<li>防止过拟合。BN层将每一个batch的均值与方差引入到网络中，由于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合。</li>
</ul>
<h4 id="BN的弊端"><a href="#BN的弊端" class="headerlink" title="BN的弊端"></a>BN的弊端</h4><ul>
<li>由于是在batch的维度进行归一化，BN层要求较大的batch才能有效地工作，而物体检测等任务由于占用内存较高，限制了batch的大小，这会限制BN层有效地发挥归一化功能。</li>
<li>数据的batch大小在训练与测试时往往不一样。在训练时一般采用滑动来计算平均值与方差，在测试时直接拿训练集的平均值与方差来使用。这种方式会导致测试集依赖于训练集，然而有时训练集与测试集的数据分布并不一致。</li>
</ul>
<h3 id="全连接层（Fully-Connected-Layers）"><a href="#全连接层（Fully-Connected-Layers）" class="headerlink" title="全连接层（Fully Connected Layers）"></a>全连接层（Fully Connected Layers）</h3><p>卷积网络的主要作用是从局部到整体地提取图像的特征，而全连接层则用来将卷积抽象出的特征图进一步映射到特定维度的标签空间，以求取损失或者输出预测结果。</p>
<h3 id="全局平均池化（Global-Average-Pooling-GAP）"><a href="#全局平均池化（Global-Average-Pooling-GAP）" class="headerlink" title="全局平均池化（Global Average Pooling, GAP）"></a>全局平均池化（Global Average Pooling, GAP）</h3><p>然而，在大模型中，全连接层的参数量会变得十分庞大，会导致网络模型应用部署困难，并且其中存在着大量的参数冗余，也容易发生过拟合的现象。现在开始有很多工作采取GAP来取代全连接层，这种思想最早见于<a href="https://arxiv.org/abs/1312.4400">NIN</a>（Network in Network）网络中，</p>
<p>一般情况下，卷积层用于提取二维数据如图片、视频等的特征，针对于具体任务（分类、回归、图像分割）等，卷积层后续会用到不同类型的网络，拿分类问题举例，最简单的方式就是将卷积网络提取出的特征（feature map）输入到softmax全连接层对应不同的类别。首先，这里的feature map是二维多通道的数据结构，类似于三个通道（红黄绿）的彩色图片，也就是这里的feature map具有空间上的信息；其次，在GAP被提出之前，常用的方式是将feature map直接拉平成一维向量（下图左），但是GAP不同，是将每个通道的二维图像做平均，最后也就是每个通道对应一个均值（下图右）。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/GAP.png" alt="GAP"></p>
<p>总体上，使用GAP有如下几点好处：</p>
<ul>
<li>利用池化实现了降维，极大减少了网络的参数量。</li>
<li>将特征提取与分类合二为一，一定程度上可以防止过拟合。</li>
<li>由于去除了全连接层，可以接受任意图像尺寸的输入。</li>
<li>赋予模型一定的可解释性</li>
</ul>
<p>但是，值得注意的是，在实践中发现使用GAP可能会导致收敛速度减慢。</p>
<h3 id="空洞卷积（Dilated-Convolution）"><a href="#空洞卷积（Dilated-Convolution）" class="headerlink" title="空洞卷积（Dilated Convolution）"></a>空洞卷积（Dilated Convolution）</h3><p><a href="https://arxiv.org/pdf/1511.07122v2.pdf">Paper</a></p>
<p>空洞卷积最初是为解决图像分割的问题而提出的。常见的图像分割算法通常使用池化层来增大感受野，同时也缩小了特征图尺寸，然后再利用上采样还原图像尺寸。特征图缩小再放大的过程造成了精度上的损失，因此需要有一种操作可以在增加感受野的同时保持特征图的尺寸不变，从而替代池化与上采样操作，在这种需求下，空洞卷积就诞生了。</p>
<p>空洞卷积，顾名思义就是卷积核中间带有一些洞，跳过一些元素进行卷积。在此以3×3卷积为例，下图中，a是普通的卷积过程，在卷积核紧密排列在特征图上滑动计算，而b代表了空洞数为2的空洞卷积，可以看到，在特征图上每2行或者2列选取元素与卷积核卷积。类似地，c代表了空洞数为3的空洞卷积。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/dilatedConv.png" alt="DilatedConv"></p>
<p>空洞卷积的优点显而易见，在不引入额外参数的前提下可以任意扩大感受野，同时保持特征图的分辨率不变。如上图中与普通3x3卷积相同的参数量，却可以达到与5x5、7x7卷积相当的感受野。这一点在分割与检测任务中十分有用，感受野的扩大可以检测大物体，而特征图分辨率不变使得物体定位更加精准。</p>
<p>当然空洞卷积也有一些缺陷：</p>
<ul>
<li>网格效应（Gridding Effect）：由于空洞卷积是一种稀疏的采样方式，当多个空洞卷积叠加时，有些像素根本没有被利用到，会损失信息的连续性与相关性，进而影响分割、检测等要求较高的任务。</li>
<li>远距离的信息没有相关性：空洞卷积采取了稀疏的采样方式，导致远距离卷积得到的结果之间缺乏相关性，进而影响分类的结果。</li>
<li>不同尺度物体的关系：大的dilation rate对于大物体分割与检测有利，但是对于小物体则有弊无利，如何处理好多尺度问题的检测，是空洞卷积设计的重点。</li>
</ul>
<p>对于上述缺陷，有多篇文章提出了不同的解决方法，典型的有HDC（Hybrid Dilated Convolution）结构。</p>
<h3 id="一些经典的CNN模型"><a href="#一些经典的CNN模型" class="headerlink" title="一些经典的CNN模型"></a>一些经典的CNN模型</h3><ul>
<li><p><a href="https://arxiv.org/pdf/1409.1556.pdf">VGGNet</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></p>
</li>
<li><a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a></li>
<li><a href="https://arxiv.org/pdf/1608.06993.pdf">DenseNet</a></li>
<li><a href="https://arxiv.org/pdf/1608.06993.pdf">FPN</a></li>
<li><a href="https://arxiv.org/pdf/1804.06215.pdf">DetNet</a></li>
</ul>
]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>面试问题（英文）</title>
    <url>/2021/12/19/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%EF%BC%88%E8%8B%B1%E6%96%87%EF%BC%89/</url>
    <content><![CDATA[<h4 id="科研相关"><a href="#科研相关" class="headerlink" title="科研相关"></a>科研相关</h4><ol>
<li><p>What is the most challenging part of academic research? (科研中最有挑战性的是什么？)</p>
<blockquote>
<p>I think deciding a topic is the most challenging. Sometimes the supervisor or senior students in the lab will give us a specific topic, that’s good, but most of the time, we should decide a topic by ourselves. There are so many things to think about. How difficult is this topic? Has anyone done this before? What is the practical significance of this project? Would this topic work? All of these need to be carefully considered and verified.</p>
</blockquote>
</li>
<li><p>How to start a new subject that you have never known about before? (如何上手一个你从未接触过的新项目？)</p>
<blockquote>
<p>Firstly, I will assess what knowledge is involved in the topic.</p>
<p>Second, if there are any aspects of knowledge that I’m not familiar to, I will search for the reviews of those areas.</p>
<p>Then, summarize the knowledge and information obtained from the investigation, and design the model and method of this research.</p>
<p>And the following is experimental validation and the paper writing.</p>
</blockquote>
</li>
</ol>
<h4 id="动机相关"><a href="#动机相关" class="headerlink" title="动机相关"></a>动机相关</h4><ol>
<li><p>Why do you think you are the right candidate for this Ph.D. program? (你为什么认为你是这个博士项目的合适人选？)</p>
<blockquote>
<p>First, I have rich academic research experience during my undergraduate study, which exercises my academic research accomplishment. Second, I have a solid professional foundation. What’s more, I’m optimistic and tenacious, and those are exactly what a researcher should have. And, I have a strong interest in the major I’m applying for, which will be my motivation to explore the unknown in the future. </p>
</blockquote>
</li>
<li><p>Why do you wish to pursue a doctoral degree? (你为什么要攻读博士学位？)</p>
<blockquote>
<p>In terms of my interest, I’m interested in AI, which field I want my future career to be in. To be a researcher, or to be an engineer that can witness the cutting-edge technology in this field. I didn’t have systematic courses about this field during my undergraduate years and my cognition in this field is still relatively shallow. A doctoral degree will give me a chance to go deeper into this field.</p>
<p>And in terms of my ambition, I want to be a person who can make a great contribution to the society, and a doctoral degree will give me the opportunity to improve myself and qualify me to be such a person.</p>
</blockquote>
</li>
<li><p>What do you plan to do after you have completed your Ph.D.? （你获得博士学位后有什么计划？）</p>
<blockquote>
<p>I feel my doctoral project can open up new lines of research for this field and want to sue it as  the foundation for a fruitful research career.</p>
<p>I hope to work as a researcher in some large technology companies such as Baidu, Tencent. They have very competitive environments, surrounded by highly skilled people. In this environment, I can continue to make progress and explore the most cutting-edge technology in this field.</p>
<p>But, I’m also interested in the wider development opportunities included in this doctoral programme. I want to be a researcher, but I’m happy to keep other options open.</p>
</blockquote>
</li>
<li><p>What impact would you like your Ph.D. project to have? (你希望你的博士项目有什么影响？)</p>
<blockquote>
<p>I know that there are plenty of excellent talents in the field that I am applying for, but I still hope  that my project can make a significant contribution to the development of this field. I want it to be innovative and </p>
</blockquote>
</li>
<li><p>Why are you interested in computer vision? （你为什么会对计算机视觉感兴趣？)</p>
<blockquote>
<p>First, computer vision is one of the hottest fields in the area of AI, most innovative methods come from here. It’s interesting and challenging.</p>
<p>Second, my past academic research experience is all about AI in life science. The problems are somehow abstract. When I finished an experiment, I could only see the scores of performance evaluation metrics. I couldn’t see the things I was researching. The tasks in computer vision are relatively straightforward, I can feel what I’m doing directly with my eyes.</p>
<p>What’s more, in the daily life, I also pay attention to some social media related to artificial intelligence, I saw many cool computer vision algorithms on them. Such as “Everybody dance now”, which can copy the dance to another people. Most of them looks very interesting.</p>
</blockquote>
</li>
<li><p>Why do you choose Westlake University? (你为什么会选择西湖大学？)</p>
<blockquote>
<p>I was preparing for the postgraduate entrance examination, but when I learned about Westlake University in detail, I realized that Westlake University is a better choice.</p>
<p>As far as I can tell, firstly, the academic level of the teachers here is very high, which is the most important. secondly, the living conditions here are very good, very attractive. There is another important point is that Westlake University provides me a larger chance to pursue what I’m interested in. </p>
<p>These are the reasons why I choose Westlake University.</p>
</blockquote>
</li>
</ol>
<h4 id="个人相关"><a href="#个人相关" class="headerlink" title="个人相关"></a>个人相关</h4><ol>
<li><p>What advantages do you see in yourself? (你觉得你有什么优点？)</p>
<blockquote>
<p>First, strong learning ability.</p>
<p>Second, I’m composed</p>
<p>Third, good at cooperation</p>
</blockquote>
</li>
<li><p>What weaknesses do you see in yourself? (你觉得你有什么缺点？)</p>
<blockquote>
<p>First, lack of urgency.</p>
<p>Second, I’m a little inarticulate.</p>
</blockquote>
</li>
<li><p>What are your strengths and weaknesses? (优点和缺点一起)</p>
<blockquote>
<p>I think I have a strong ability to learn now knowledge. When I meet a new project, I can always quickly learn the required pre-knowledge and get started with the work. But I’m not good at summarizing the knowledge I have learned, so many times I need to repeatedly inquire them. However, in order to overcome this weakness, I recently set up my personal blog, hoping to cultivate my knowledge summary and written expression ability while running my personal blog, so that become a knowledge exporter.</p>
</blockquote>
</li>
<li><p>What are your future career plans? (你未来的职业规划？)</p>
<blockquote>
<p>I hope to work as a researcher in some large technology companies such as Baidu, Tencent. That’s a very competitive environment, surrounded by highly skilled people. In this environment, I can continue to make progress and explore the most cutting-edge technology in this field.</p>
</blockquote>
</li>
<li><p>What are your plans for your doctoral life? (你对博士生期间生活的规划是怎么样的？)</p>
<blockquote>
<p>If I can be admitted, I will learn the basic knowledge of this major and think of my detailed research topic before enrollment so that I can participate in the research work of lab as soon as possible. </p>
<p>After enrollment, I will keep up with the latest developments in related research fields and read the latest high-level papers. And during the first three or four years of my doctorate, I will try to produce academic research results. I don’t expect them to be many, but I want them to be high-quality and creative.</p>
<p>And in the last year of my doctorate, I will complete my doctoral thesis in a high standard.</p>
<p>What’s more, in my spare time, I will actively participate in various activities held by the school and fully enjoy my postgraduate life.</p>
</blockquote>
</li>
<li><p>What’s the most challenging thing you’ve done so far, and how did you overcome it? (截止到目前做的最有挑战性的事情是什么？你是怎么克服它的？)</p>
<blockquote>
<p>I think the most challenging thing I’ve done is the second academic research experience I mentioned in the PPT. Before the project, my programming skills were very poor and my knowledge of artificial intelligence algorithms was very basic. Since the project was undertaken by me alone, the beginning was difficult. There was a lot of things to learn, there were always bug existing in the code, the results of experiments were always unsatisfactory. But this process was not painful, on the contrary, I got a sense of pleasure and achievement from overcoming these difficulties and learning new knowledge.</p>
</blockquote>
</li>
</ol>
<h4 id="项目相关"><a href="#项目相关" class="headerlink" title="项目相关"></a>项目相关</h4><ol>
<li><p>What do you think is the most creative part in your research? (你认为你的研究项目中最有创造性的部分是什么？)</p>
<blockquote>
<p>I think the most creative part in my research is that, based on the assumption that one methylation site should have similar expression values in the same class of samples and different values in different class of samples, I designed a method to abstract the methylation patterns into the structure of graphs. There is a transformation from “clinical sample as main subject and the values of its methylation sites as features” to “methylation sites as main subject and the values of it on different clinical samples as features”. I think it’s a key step so that we can use graph neural network to extract the differentially expressed methylation sites.</p>
</blockquote>
</li>
<li><p>Why do you use SAGPooling in your work?</p>
<blockquote>
<p>At the beginning of this research, we designed the main idea of the study, and at the main time, I was learning how to realize the programming of GNN. There is a package of python named <code>pytorch geometric</code> which is designed specifically for graph neural networks based on pytorch.</p>
<p>There was a function named SAGPooling. I wanted to know what it did, so I read the paper of this method that was attached to the website of the package. </p>
<p>After reading, I thought, Wow, it’s exactly the method that perfectly fit our idea. </p>
<p>Fortunately this method did perform well in this research.</p>
</blockquote>
</li>
<li><p>Is any other with you in this research?</p>
<blockquote>
<p>There is a postdoctor with me. But because she has many other projects, I did all the major work of this project. All the programming, all the experiment. During the first half of this year, I updated her regularly on my progress, and in the second half of this year, when the project came to an end, she edited the manuscript with me.</p>
</blockquote>
</li>
<li><p>How do you extract the methylation sites?</p>
<blockquote>
<p>It is mentioned before that there are 3 successive modules in the model. We fixed the pooling rate of the first two modules as 0.5 and change that of the last module to change the number of methylation sites kept finally. Through experiments, we found that the accuracy of graph classification was the highest when the pooling rate of the third module was set as 0.5. So the number of finally kept nodes was 250. </p>
<p>And theoratically, the </p>
</blockquote>
</li>
</ol>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>西湖大学</tag>
      </tags>
  </entry>
</search>
