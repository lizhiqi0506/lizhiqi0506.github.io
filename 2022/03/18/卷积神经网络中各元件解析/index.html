<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>卷积神经网络中各元件解析 | Akinoike</title><meta name="keywords" content="Pytorch,CNN"><meta name="author" content="Zhiqi Li"><meta name="copyright" content="Zhiqi Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Conv2d参数含义 in_channels: 输入特征图的通道数，如果是RGB图像，则通道数为3。卷积中的特征图通道数一般是2的整数次幂。 out_channels: 输出特征图的通道数。 kernel_size: 卷积核的尺寸，常见的有1，3，5，7. stride: 步长，一般为1。如果大于1，则输出特征图的尺寸会小于输出特征图的尺寸。 padding：填充，填充方式有constant（常量">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络中各元件解析">
<meta property="og:url" content="http://example.com/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="Akinoike">
<meta property="og:description" content="Conv2d参数含义 in_channels: 输入特征图的通道数，如果是RGB图像，则通道数为3。卷积中的特征图通道数一般是2的整数次幂。 out_channels: 输出特征图的通道数。 kernel_size: 卷积核的尺寸，常见的有1，3，5，7. stride: 步长，一般为1。如果大于1，则输出特征图的尺寸会小于输出特征图的尺寸。 padding：填充，填充方式有constant（常量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/defaultcover7.jpg">
<meta property="article:published_time" content="2022-03-18T11:34:22.000Z">
<meta property="article:modified_time" content="2022-03-19T10:28:00.839Z">
<meta property="article:author" content="Zhiqi Li">
<meta property="article:tag" content="Pytorch">
<meta property="article:tag" content="CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/defaultcover7.jpg"><link rel="shortcut icon" href="/img/learning2.png"><link rel="canonical" href="http://example.com/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '卷积神经网络中各元件解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-19 18:28:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wanye.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/defaultcover7.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Akinoike</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">卷积神经网络中各元件解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-18T11:34:22.000Z" title="发表于 2022-03-18 19:34:22">2022-03-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-03-19T10:28:00.839Z" title="更新于 2022-03-19 18:28:00">2022-03-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="卷积神经网络中各元件解析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><h4 id="参数含义"><a href="#参数含义" class="headerlink" title="参数含义"></a>参数含义</h4><ul>
<li>in_channels: 输入特征图的通道数，如果是RGB图像，则通道数为3。卷积中的特征图通道数一般是2的整数次幂。</li>
<li>out_channels: 输出特征图的通道数。</li>
<li>kernel_size: 卷积核的尺寸，常见的有1，3，5，7.</li>
<li>stride: 步长，一般为1。如果大于1，则输出特征图的尺寸会小于输出特征图的尺寸。</li>
<li>padding：填充，填充方式有constant（常量填充，默认情况下是0填充），reflect（反射填充）、replicate（复制填充）、circular（循环填充）</li>
<li>dilation: 空洞卷积，当大于1时可以增大感受野的同时保持特征图的尺寸，默认为1。</li>
<li>groups: 组卷积，即在卷积操作时不是逐点卷积，而是将输入通道分为多个组，稀疏连接打到降低计算量的目的，默认为1.</li>
<li>bias：是否需要偏置。</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer,">Dropout Paper</a></li>
</ul>
<p>2012年，Hinton等人提出了Dropout算法，可以比较有效地缓解过拟合现象，起到一定正则化的效果。</p>
<p>Dropout的基本思想如图所示，在训练时，每个神经元以概率p保留，即以1-p的概率停止工作，每次前向传播保留下来的神经元都不同，这样可以使得模型不太依赖于某些局部特征，泛化性能更强。在测试时，为了保证相同的输出期望值，每个参数还要乘以p。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/dropout.png" alt="Dropout"></p>
<p>为什么Dropout可以防止过拟合，可以从以下3个方面解释：</p>
<ul>
<li>多模型的平均：不同的固定神经网络会有不同的过拟合，多个取平均则有可能让一些相反的拟合抵消掉，而Dropout每次都是不同的神经元失活，可以看做是多个模型的平均，类似于多数投票取胜的策略。</li>
<li>减少神经元间的依赖：由于两个神经元不一定同时有效，因此减少了特征之间的依赖，迫使网络学习有更为鲁棒的特征，因为神经网络不应该对特定的特征敏感，而应该从众多特征中学习更为共同的规律，这也起到了正则化的效果。</li>
<li>生物进化：Dropout类似于性别在生物进化中的角色，物种为了适应环境变化，在繁衍时取雄性和雌性的各一半基因进行组合，这样可以适应更复杂的新环境，避免了单一基因的过拟合，当环境发生变化时也不至于灭绝。</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul>
<li><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/ioffe15.pdf">BN Paper</a></li>
</ul>
<p>由于网络中参数变化导致的内部节点数据分布发生变化的现象被称做ICS（Internal Covariate Shift）。ICS现象容易使训练过程陷入饱和区，减慢网络的收敛。前面提到的ReLU从激活函数的角度出发，在一定程度上解决了梯度饱和的现象，而2015年提出的BN层，则从改变数据分布的角度避免了参数陷入饱和区。由于BN层优越的性能，其已经是当前卷积网络中的“标配”。</p>
<h4 id="BN的过程"><a href="#BN的过程" class="headerlink" title="BN的过程"></a>BN的过程</h4><p>对于一个batch的输入$x: B=\{x_1, …, x_m\}$，其中$m$为batch的大小，BN首先对其进行白化操作，即去均值方差的过程。</p>
<script type="math/tex; mode=display">
\mu_B=\frac{1}{m}\sum_{i=1}^{m}x_i \\
\sigma^2_B = \frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_B)^2 \\
\hat{x}_i = \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}</script><p>其中$\epsilon$是为了数值稳定而添加的一个常数（论文原文是”a constant added to the mini-batch variance for numerical stability“，这里我也不明白为什么这样就可以使数值更稳定）</p>
<p>白化操作可以使输入的特征分布具有相同的均值与方差，固定了每一层的输入分布，从而加速网络的收敛。但是，白化操作也限制了网络中数据的表达能力，浅层学到的参数信息会被白化操作屏蔽掉，因此，BN层在白化操作后又增加了一个可学习的线性变换操作（引入可学习参数$\gamma$和$\beta$），让数据尽可能地恢复本身的表达能力：</p>
<script type="math/tex; mode=display">
y_i=\gamma\hat{x}_i+\beta</script><h4 id="BN的优点"><a href="#BN的优点" class="headerlink" title="BN的优点"></a>BN的优点</h4><ul>
<li>缓解梯度消失，加速网络收敛。BN层可以让激活函数的输入数据落在非饱和区，缓解了梯度消失问题。此外，由于每一层数据的均值与方差都在一定范围内，深层网络不必去不断适应浅层网络输入的变化，实现了层间解耦，允许每一层独立学习，也加快了网络的收敛。</li>
<li>简化调参，网络更稳定。在调参时，学习率调得过大容易出现震荡与不收敛，BN层则抑制了参数微小变化随网络加深而被放大的问题，因此对于参数变化的适应能力更强，更容易调参。</li>
<li>防止过拟合。BN层将每一个batch的均值与方差引入到网络中，由于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合。</li>
</ul>
<h4 id="BN的弊端"><a href="#BN的弊端" class="headerlink" title="BN的弊端"></a>BN的弊端</h4><ul>
<li>由于是在batch的维度进行归一化，BN层要求较大的batch才能有效地工作，而物体检测等任务由于占用内存较高，限制了batch的大小，这会限制BN层有效地发挥归一化功能。</li>
<li>数据的batch大小在训练与测试时往往不一样。在训练时一般采用滑动来计算平均值与方差，在测试时直接拿训练集的平均值与方差来使用。这种方式会导致测试集依赖于训练集，然而有时训练集与测试集的数据分布并不一致。</li>
</ul>
<h3 id="全连接层（Fully-Connected-Layers）"><a href="#全连接层（Fully-Connected-Layers）" class="headerlink" title="全连接层（Fully Connected Layers）"></a>全连接层（Fully Connected Layers）</h3><p>卷积网络的主要作用是从局部到整体地提取图像的特征，而全连接层则用来将卷积抽象出的特征图进一步映射到特定维度的标签空间，以求取损失或者输出预测结果。</p>
<h3 id="全局平均池化（Global-Average-Pooling-GAP）"><a href="#全局平均池化（Global-Average-Pooling-GAP）" class="headerlink" title="全局平均池化（Global Average Pooling, GAP）"></a>全局平均池化（Global Average Pooling, GAP）</h3><p>然而，在大模型中，全连接层的参数量会变得十分庞大，会导致网络模型应用部署困难，并且其中存在着大量的参数冗余，也容易发生过拟合的现象。现在开始有很多工作采取GAP来取代全连接层，这种思想最早见于<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.4400">NIN</a>（Network in Network）网络中，</p>
<p>一般情况下，卷积层用于提取二维数据如图片、视频等的特征，针对于具体任务（分类、回归、图像分割）等，卷积层后续会用到不同类型的网络，拿分类问题举例，最简单的方式就是将卷积网络提取出的特征（feature map）输入到softmax全连接层对应不同的类别。首先，这里的feature map是二维多通道的数据结构，类似于三个通道（红黄绿）的彩色图片，也就是这里的feature map具有空间上的信息；其次，在GAP被提出之前，常用的方式是将feature map直接拉平成一维向量（下图左），但是GAP不同，是将每个通道的二维图像做平均，最后也就是每个通道对应一个均值（下图右）。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/GAP.png" alt="GAP"></p>
<p>总体上，使用GAP有如下几点好处：</p>
<ul>
<li>利用池化实现了降维，极大减少了网络的参数量。</li>
<li>将特征提取与分类合二为一，一定程度上可以防止过拟合。</li>
<li>由于去除了全连接层，可以接受任意图像尺寸的输入。</li>
<li>赋予模型一定的可解释性</li>
</ul>
<p>但是，值得注意的是，在实践中发现使用GAP可能会导致收敛速度减慢。</p>
<h3 id="空洞卷积（Dilated-Convolution）"><a href="#空洞卷积（Dilated-Convolution）" class="headerlink" title="空洞卷积（Dilated Convolution）"></a>空洞卷积（Dilated Convolution）</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.07122v2.pdf">Paper</a></p>
<p>空洞卷积最初是为解决图像分割的问题而提出的。常见的图像分割算法通常使用池化层来增大感受野，同时也缩小了特征图尺寸，然后再利用上采样还原图像尺寸。特征图缩小再放大的过程造成了精度上的损失，因此需要有一种操作可以在增加感受野的同时保持特征图的尺寸不变，从而替代池化与上采样操作，在这种需求下，空洞卷积就诞生了。</p>
<p>空洞卷积，顾名思义就是卷积核中间带有一些洞，跳过一些元素进行卷积。在此以3×3卷积为例，下图中，a是普通的卷积过程，在卷积核紧密排列在特征图上滑动计算，而b代表了空洞数为2的空洞卷积，可以看到，在特征图上每2行或者2列选取元素与卷积核卷积。类似地，c代表了空洞数为3的空洞卷积。</p>
<p><img src="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/dilatedConv.png" alt="DilatedConv"></p>
<p>空洞卷积的优点显而易见，在不引入额外参数的前提下可以任意扩大感受野，同时保持特征图的分辨率不变。如上图中与普通3x3卷积相同的参数量，却可以达到与5x5、7x7卷积相当的感受野。这一点在分割与检测任务中十分有用，感受野的扩大可以检测大物体，而特征图分辨率不变使得物体定位更加精准。</p>
<p>当然空洞卷积也有一些缺陷：</p>
<ul>
<li>网格效应（Gridding Effect）：由于空洞卷积是一种稀疏的采样方式，当多个空洞卷积叠加时，有些像素根本没有被利用到，会损失信息的连续性与相关性，进而影响分割、检测等要求较高的任务。</li>
<li>远距离的信息没有相关性：空洞卷积采取了稀疏的采样方式，导致远距离卷积得到的结果之间缺乏相关性，进而影响分类的结果。</li>
<li>不同尺度物体的关系：大的dilation rate对于大物体分割与检测有利，但是对于小物体则有弊无利，如何处理好多尺度问题的检测，是空洞卷积设计的重点。</li>
</ul>
<p>对于上述缺陷，有多篇文章提出了不同的解决方法，典型的有HDC（Hybrid Dilated Convolution）结构。</p>
<h3 id="一些经典的CNN模型"><a href="#一些经典的CNN模型" class="headerlink" title="一些经典的CNN模型"></a>一些经典的CNN模型</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">VGGNet</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></p>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.06993.pdf">DenseNet</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.06993.pdf">FPN</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.06215.pdf">DetNet</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Zhiqi Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/">http://example.com/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Akinoike</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="post_share"><div class="social-share" data-image="/img/defaultcover7.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/01/13/2022-01-13-%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%AE%B0-@%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8/"><img class="next-cover" src="/img/python.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">编程小记-&quot;@&quot;运算符的作用</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wanye.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhiqi Li</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lizhiqi0506"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lizhiqi0506" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lizhiqi0506@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Conv2d"><span class="toc-number">1.</span> <span class="toc-text">Conv2d</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%90%AB%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">参数含义</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">2.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.</span> <span class="toc-text">Batch Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BN%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">BN的过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">3.2.</span> <span class="toc-text">BN的优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN%E7%9A%84%E5%BC%8A%E7%AB%AF"><span class="toc-number">3.3.</span> <span class="toc-text">BN的弊端</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layers%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">全连接层（Fully Connected Layers）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%EF%BC%88Global-Average-Pooling-GAP%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">全局平均池化（Global Average Pooling, GAP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%EF%BC%88Dilated-Convolution%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">空洞卷积（Dilated Convolution）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.</span> <span class="toc-text">一些经典的CNN模型</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/" title="卷积神经网络中各元件解析"><img src="/img/defaultcover7.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络中各元件解析"/></a><div class="content"><a class="title" href="/2022/03/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%90%84%E5%85%83%E4%BB%B6%E8%A7%A3%E6%9E%90/" title="卷积神经网络中各元件解析">卷积神经网络中各元件解析</a><time datetime="2022-03-18T11:34:22.000Z" title="发表于 2022-03-18 19:34:22">2022-03-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/13/2022-01-13-%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%AE%B0-@%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8/" title="编程小记-&quot;@&quot;运算符的作用"><img src="/img/python.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="编程小记-&quot;@&quot;运算符的作用"/></a><div class="content"><a class="title" href="/2022/01/13/2022-01-13-%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%AE%B0-@%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8/" title="编程小记-&quot;@&quot;运算符的作用">编程小记-&quot;@&quot;运算符的作用</a><time datetime="2022-01-12T16:00:00.000Z" title="发表于 2022-01-13 00:00:00">2022-01-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/10/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2/" title="MVG-学习笔记-射影变换"><img src="/img/MVG.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MVG-学习笔记-射影变换"/></a><div class="content"><a class="title" href="/2022/01/10/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2/" title="MVG-学习笔记-射影变换">MVG-学习笔记-射影变换</a><time datetime="2022-01-10T05:09:54.876Z" title="发表于 2022-01-10 13:09:54">2022-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/09/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%87%A0%E4%BD%95%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="MVG-学习笔记-射影几何基础概念"><img src="/img/MVG.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MVG-学习笔记-射影几何基础概念"/></a><div class="content"><a class="title" href="/2022/01/09/MVG-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%84%E5%BD%B1%E5%87%A0%E4%BD%95%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="MVG-学习笔记-射影几何基础概念">MVG-学习笔记-射影几何基础概念</a><time datetime="2022-01-09T06:19:02.000Z" title="发表于 2022-01-09 14:19:02">2022-01-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/05/history-of-cv/" title="history_of_cv"><img src="/img/digital_eye.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="history_of_cv"/></a><div class="content"><a class="title" href="/2022/01/05/history-of-cv/" title="history_of_cv">history_of_cv</a><time datetime="2022-01-05T11:25:11.000Z" title="发表于 2022-01-05 19:25:11">2022-01-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Zhiqi Li</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'YSiJOKeyzKHPzFtn7zSbLPTH-gzGzoHsz',
      appKey: 'pdRWVedTTODVqgPz0CQfTers',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: true,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: true,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>